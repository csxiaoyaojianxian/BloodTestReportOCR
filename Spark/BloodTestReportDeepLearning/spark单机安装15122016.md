    下面将介绍Apache Spark 1.6.0在单机的部署，与在集群中部署的步骤基本一致，只是少了一些master和slave文件的配置。
    
    0.Spark的安装准备
    我的电脑环境是Ubuntu 16.04 LTS，还需要安装：
    jdk-8u112-linux-x64.tar        网址：http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html
    hadoop-2.7.3.tar.gz            网址：http://apache.claz.org/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
    scala-2.12.1.tgz               网址：http://www.scala-lang.org/download/2.10.6.html
    spark-2.0.2-bin-hadoop2.7.tgz  网址：http://spark.apache.org/downloads.html
    
    
    （以下安装均在普通用户下安装，username代表你自己的用户名）
    1.安装jdk
    mkdir /home/username/tom
    cd /home/username/tom
    $ tar -xzvf jdk-8u112-linux-x64.tar.gz
    $ sudo vim /etc/profile
    
    编辑/etc/profile文件，在最后加上java环境变量：
    export JAVA_HOME=/home/username/tom/jdk1.8.0_112/
    export JRE_HOME=/home/username/tom/jdk1.8.0_112/jre
    export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH
    export CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH
    
    保存并更新 /etc/profile    
    $ source /etc/profil
    
    查看是否成功：
    $ java -version


    2.配置ssh localhost
    
    确保安装好ssh:  
    $ sudo apt-get update
    $ sudo apt-get install openssh-server
    $ sudo /etc/init.d/ssh start
    
    生成并添加密钥： 
    $ ssh-keygen -t rsa
    $ cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    $ chmod 0600 ~/.ssh/authorized_keys
    
    如果已经生成过密钥，只需执行后两行命令。测试ssh localhost  
    $ ssh localhost
    $ exit


    3.安装hadoop2.7.3
    
    解压hadoop2.7.3到任意目录：  
    $ cd /home/username/tom
    $ tar -xzvf hadoop-2.7.3.tar.gz
    
    编辑 /etc/profile 文件 
    export HADOOP_HOME=/home/username/tom/hadoop-2.7.3
    export HADOOP_INSTALL=$HADOOP_HOME
    export HADOOP_MAPRED_HOME=$HADOOP_HOME
    export HADOOP_COMMON_HOME=$HADOOP_HOME
    export HADOOP_HDFS_HOME=$HADOOP_HOME
    export YARN_HOME=$HADOOP_HOME
    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
    export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
    
    保存并更新 /etc/profile ：
    $ source /etc/profil
    
    编辑 $HADOOP_HOME/etc/hadoop/hadoop-env.sh 文件   
    $ vim $HADOOP_HOME/etc/hadoop/hadoop-env.sh
    
    在最后加上：   
    export JAVA_HOME=/home/username/tom/jdk1.8.0_112/
    
    修改Configuration文件：  
    $ cd $HADOOP_HOME/etc/hadoop
    
    修改 core-site.xml ：  
    <configuration>
    <property>
      <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
    </property>
    </configuration>

    修改 hdfs-site.xml ：
    <configuration>
    <property>
      <name>dfs.replication</name>
      <value>1</value>
    </property>

    <property>
      <name>dfs.name.dir</name>
        <value>file:///home/username/tom/hadoopdata/hdfs/namenode</value>
    </property>

    <property>
       <name>dfs.data.dir</name>
         <value>file:///home/username/tom/hadoopdata/hdfs/datanode</value>
    </property>
    </configuration>
    
    第一个是dfs的备份数目，单机用1份就行，后面两个是namenode和datanode的目录。
    
    修改 mapred-site.xml ：   
    <configuration>
     <property>
      <name>mapreduce.framework.name</name>
      <value>yarn</value>
     </property>
    </configuration>
    
    修改 yarn-site.xml ：    
    <configuration>
      <property>
       <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
      </property>
    </configuration>
    
    初始化hadoop：
    $ hdfs namenode -format
    
    启动
    $ $HADOOP_HOME/sbin/start-all.sh
    
    停止   
    $ $HADOOP_HOME/sbin/stop-all.sh
    
    检查WebUI，浏览器打开端口： http://localhost :8088
    port 8088: cluster and all applications
    port 50070: Hadoop NameNode
    port 50090: Secondary NameNode
    port 50075: DataNode
    
    hadoop运行后可使用 jps 命令查看,得到结果：
    10057 Jps
    9611 ResourceManager
    9451 SecondaryNameNode
    9260 DataNode
    9102 NameNode
    9743 NodeManager
    
    
    4.安装scala
    
    解压scala安装包到任意目录：
    $ cd /home/username/tom
    $ tar -xzvf scala-2.12.1.tgz
    $ sudo vim /etc/profile

    在 /etc/profile 文件的末尾添加环境变量：
    export SCALA_HOME=/home/username/tom//scala-2.12.1
    export PATH=$SCALA_HOME/bin:$PATH

    保存并更新 /etc/profile ：
    $ source /etc/profil
    
    查看是否成功：
    $ scala -version
    
    
    5.安装Spark
    
    解压spark安装包到任意目录：
    $ cd /home/username/tom
    $ tar -xzvf spark-2.0.2-bin-hadoop2.7.tgz
    $ mv spark-2.0.2-bin-hadoop2.7.tgz spark-2.0.2
    $ sudo vim /etc/profile
    
    在 /etc/profile 文件的末尾添加环境变量：
    export SPARK_HOME=/home/username/tom/spark-2.0.2
    export PATH=$SPARK_HOME/bin:$PATH
    
    保存并更新 /etc/profile ：
    $ source /etc/profil
    
    在conf目录下复制并重命名 spark-env.sh.template 为 spark-env.sh ：
    $ cp spark-env.sh.template spark-env.sh
    $ vim spark-env.sh
    
    在 spark-env.sh 中添加：
    export JAVA_HOME=/home/username/tom/jdk1.8.0_112/
    export SCALA_HOME=/home/username/tom//scala-2.12.1
    export SPARK_MASTER_IP=localhost
    export SPARK_WORKER_MEMORY=4G  #WORKER MEMORY大小,根据电脑情况设置
    
    启动
    $ $SPARK_HOME/sbin/start-all.sh
    
    停止
    $ $SPARK_HOME/sbin/stop-all.sh
    
    测试Spark是否安装成功：
    $ $SPARK_HOME/bin/run-example SparkPi
    
    得到结果：
    Pi is roughly 3.14716
    
    检查WebUI，浏览器打开端口： http://localhost :8080
    
    
    00.配置python
    我的Python版本是python2.7.12
    
    # Ubuntu/Linux 64-bit
    $ sudo apt-get install python-pip python-dev
    $ sudo apt-get install python-numpy  #安装Python依赖包
    
    设置Python依赖路径
    sudo vim /etc/profile
    
    在结尾处添加
    export SPARK_HOME=/home/username/tom/spark-2.0.2  #你的Spark解压目录(配置过spark,此项可以省略)
    export PYTHONPATH=$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.3-src.zip:$PYTHONPATH   
    #py4j及pysqrk的相关依赖路径,py4j-0.10.3-src文件名可能会因Spark版本不同而不同，请设置为自己对应目录下的文件名
    